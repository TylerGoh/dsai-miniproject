import pandas as pd
import numpy as np
import seaborn as sb
import torch
from IPython.display import Image
from transformers import AutoTokenizer, BertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import mean_absolute_error
from torch.utils.data import Dataset, DataLoader
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from catboost import CatBoostClassifier
pd.options.display.max_columns = None  # Shows all columns when printing a dataframe
sb.set()


Image(filename='./data/dataschema.png', width=800, height=800)


Customers = pd.read_csv('./data/olist_customers_dataset.csv')
Geolocation = pd.read_csv('./data/olist_geolocation_dataset.csv')
Orders = pd.read_csv('./data/olist_orders_dataset.csv')
OrderItems = pd.read_csv('./data/olist_order_items_dataset.csv')
OrderPayments = pd.read_csv('./data/olist_order_payments_dataset.csv')
OrderReviews = pd.read_csv('./data/olist_order_reviews_dataset.csv')
Products = pd.read_csv('./data/olist_products_dataset.csv')
Sellers = pd.read_csv('./data/olist_sellers_dataset.csv')


Geolocation.head()


Geolocation.drop_duplicates(subset=['geolocation_zip_code_prefix'], inplace=True)


print(Customers.info())
print(Geolocation.info())


Customers.rename(columns={"customer_zip_code_prefix": "zip_code"}
                 , inplace=True)
Geolocation.rename(columns={"geolocation_zip_code_prefix": "zip_code"}
                   , inplace=True)


train_data = Customers.merge(Orders, on="customer_id")
train_data = train_data.merge(OrderReviews, on="order_id")
train_data = train_data.merge(OrderItems, on="order_id")
train_data = train_data.merge(OrderPayments, on="order_id")
train_data = train_data.merge(Products, on="product_id")
train_data = train_data.merge(Sellers, on="seller_id")
train_data = train_data.merge(Geolocation, on="zip_code", how="left")


train_data.info()


train_data.drop(columns=["review_comment_title",
                         "customer_id",
                         "customer_unique_id",
                         "zip_code",
                         "order_id",
                         "order_status",
                         "review_id",
                         'order_approved_at',
                         "review_creation_date",
                         "review_answer_timestamp",
                         "order_item_id",
                         'order_delivered_carrier_date',
                         "product_id",
                         "seller_id",
                         "shipping_limit_date",
                         "payment_sequential",
                         "payment_type",
                         "payment_installments",
                         "product_name_lenght",
                         "product_description_lenght",
                         "geolocation_city",
                         "geolocation_state",
                         "seller_zip_code_prefix"], inplace=True)


cleaned_data = train_data.dropna().copy()
cleaned_data.info()


# Converting delivery times from string to datetime format
cleaned_data["order_purchase_timestamp"] = cleaned_data["order_purchase_timestamp"].astype('datetime64[ns]')
cleaned_data["order_delivered_customer_date"] = cleaned_data["order_delivered_customer_date"].astype('datetime64[ns]')
cleaned_data["order_estimated_delivery_date"] = cleaned_data["order_estimated_delivery_date"].astype('datetime64[ns]')
cleaned_data.tail()


# Creating new features based on current features
feature_data = cleaned_data.copy()
feature_data['days_to_deliver'] = (feature_data['order_delivered_customer_date'] - feature_data['order_purchase_timestamp']).dt.days
feature_data['days_before_estimated_delivery'] = (feature_data['order_estimated_delivery_date'] - feature_data['order_delivered_customer_date']).dt.days
feature_data.drop(columns=["order_purchase_timestamp", "order_delivered_customer_date", "order_estimated_delivery_date"], inplace=True)


feature_data.tail()


def get_volume(row):
    volume = row["product_length_cm"] * row["product_height_cm"] * row["product_width_cm"]
    return pd.Series(volume)


feature_data[['volume']] = feature_data.apply(get_volume, axis=1)
feature_data.drop(columns=["product_length_cm", "product_height_cm", "product_width_cm"], inplace=True)
feature_data.tail()


def normalize_column(data, maximum=1):
    data_min = data.min()[0]
    data_max = data.max()[0]
    new_min = 0
    new_max = maximum
    mapper = lambda x: ((x - data_min) / (data_max - data_min)) * (new_max - new_min) + new_min
    return data.apply(mapper)


normalized_data = feature_data.copy()
normalized_data["volume"] = normalize_column(normalized_data[["volume"]])
normalized_data["product_weight_g"] = normalize_column(normalized_data[["product_weight_g"]])
normalized_data["payment_value"] = normalize_column(normalized_data[["payment_value"]])
normalized_data["freight_value"] = normalize_column(normalized_data[["freight_value"]])
normalized_data["price"] = normalize_column(normalized_data[["price"]])
normalized_data.head()


latitude = normalized_data[['geolocation_lat']]
longitude = normalized_data[['geolocation_lng']]
lat_range = latitude.max()[0] - latitude.min()[0]
long_range = longitude.max()[0] - longitude.min()[0]
max_range = max(lat_range, long_range)
print(f"Latitude range: {lat_range}, Longitude range: {long_range}")

normalized_data["longitude"] = normalize_column(longitude, maximum=lat_range/max_range)
normalized_data["latitude"] = normalize_column(latitude, maximum=lat_range/max_range)
normalized_data.drop(columns=["geolocation_lat", "geolocation_lng"], inplace=True)
normalized_data.head()


tokenizer = AutoTokenizer.from_pretrained("./model")
finbertptbr = BertForSequenceClassification.from_pretrained("./model")


def apply_sentiment(row):
    tokens = tokenizer([row["review_comment_message"]], return_tensors="pt",
                       padding=True, truncation=True, max_length=512)
    output = finbertptbr(**tokens).logits.cpu().detach().numpy()[0]
    return pd.Series([output[0], output[1], output[2]])


normalized_data[['Postive', 'Negative', 'Neutral']] = normalized_data.apply(apply_sentiment, axis=1)
normalized_data.drop(columns=["review_comment_message"], inplace=True)
normalized_data.to_csv('sentiment_applied_data.csv', index=False)
normalized_data.head()


normalized_data = pd.read_csv('sentiment_applied_data.csv')
normalized_data.corr()


encoded_data = normalized_data.copy()
encoded_data


def oneHotEncode(data, column):
    data[column].str.replace(" ", "")
    newColumns = pd.get_dummies(data[column], prefix=column+"_")
    data = pd.concat([data, newColumns], axis=1)
    return data


encoded_data = oneHotEncode(encoded_data, "customer_city")
encoded_data = oneHotEncode(encoded_data, "customer_state")
encoded_data = oneHotEncode(encoded_data, "seller_state")
encoded_data = oneHotEncode(encoded_data, "seller_city")
encoded_data = oneHotEncode(encoded_data, "product_category_name")
encoded_data.drop(columns=["customer_city",
                           "customer_state",
                           "seller_state",
                           "seller_city",
                           "product_category_name"], inplace=True)
encoded_data.head()


y = encoded_data[["review_score"]]
X = encoded_data.drop(columns=["review_score"])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)
dectree = DecisionTreeClassifier(max_depth=10)
dectree.fit(X_train, y_train)


score = dectree.score(X_test, y_test)
print(f'The accuracy is {score*100:.2f}%')


modelcat = CatBoostClassifier(iterations=30,
                              learning_rate=1,
                              depth=13)
modelcat.fit(X_train, y_train)


modelcat.plot_tree(
    tree_idx=0,
)


y_pred = modelcat.predict(X_test)
y_pred = (np.rint(y_pred)).astype(int)
score = accuracy_score(y_pred, y_test)
print(f'The accuracy is {score*100:.2f}%')


y = normalized_data[["review_score",]]
X = normalized_data.drop(columns=["review_score", "customer_city", "customer_state", "seller_state", "seller_city", "product_category_name"])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)
modelcat = CatBoostClassifier(iterations=30,
                              learning_rate=1,
                              depth=16)
modelcat.fit(X_train, y_train)


y_pred = modelcat.predict(X_test)
y_pred = (np.rint(y_pred)).astype(int)
score = accuracy_score(y_pred, y_test)
print(f'The accuracy is {score*100:.2f}%')


y_train_onehot = pd.get_dummies(y_train.loc[:, 'review_score'])
y_train_onehot


class Data(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X.to_numpy().astype(np.float32))
        self.y = torch.tensor(y.to_numpy().astype(np.float32))
        self.len = self.X.shape[0]

    def __getitem__(self, index):
        return self.X[index], self.y[index]

    def __len__(self):
        return self.len


train_data = Data(X_train, y_train_onehot)
train_dataloader = DataLoader(dataset=train_data, batch_size=32, shuffle=True)
print(train_data.X)
print(train_data.y)


class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(13, 130)
        self.fc2 = nn.Linear(130, 65)
        self.fc3 = nn.Linear(65, 39)
        self.fc4 = nn.Linear(39, 5)
        self.sm = nn.Softmax(dim=1)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        x = self.sm(x)
        return x


net = Net()
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)


for epoch in range(30):  # loop over the dataset multiple times
    running_loss = 0.0
    for batch, (X, y) in enumerate(train_dataloader):
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward + backward + optimize
        outputs = net(X)
        loss = criterion(outputs, y)
        loss.backward()
        optimizer.step()
        # print statistics
        running_loss += loss.item()
    print(f'[{epoch + 1}] loss: {running_loss / 2000:.3f}')
    running_loss = 0.0

print(outputs[0:3])
print(y[0:3])
print('Finished Training')


outputs = net(torch.tensor(X_test.to_numpy().astype(np.float32)))
_, y_pred = torch.max(outputs.data, 1)
score = accuracy_score(y_pred+1, y_test)
print(f'The accuracy is {score*100:.2f}%')
